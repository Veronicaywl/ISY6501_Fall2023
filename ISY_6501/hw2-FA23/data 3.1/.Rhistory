install.packages("tidyverse")
install.packages("jsonlite")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
students <- c("Abraham", "Beatrice", "Cory", "Dinah", "Eric", "Felicia")
roll_call <- function(class){
print(Sys.Date())
for (student in class){
print(student)
}
}
roll_call(students)
locker_combinations <- function(class){
for (student in class){
combination <- sample(33,3)
print(student)
print(combination)
}
}
locker_combinations(students)
my_data <- read.delim(credit_card_data-headers.txt)
my_data <- read.delim("credit_card_data-headers.txt", header = TRUE, sep = ",",dec = ",")
setwd("~/Desktop/hw2-FA23/data 3.1")
knitr::opts_chunk$set(echo = TRUE)
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
split_sample <- createDataPartition(cc_data$V11, p = 0.8, list = FALSE)
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
library(corrplot)
library(NbClust)
library(ISLR)
library(factoextra)
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
library(corrplot)
library(NbClust)
library(ISLR)
library(factoextra)
install.packages("caret")
install.packages("caret")
install.packages("caret")
knitr::opts_chunk$set(echo = TRUE)
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
library(corrplot)
library(NbClust)
library(ISLR)
library(factoextra)
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
split_sample <- createDataPartition(cc_data$V11, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
train_data = cc_data[split_sample,]
#Assign the testData set to the remaining 20% of the original data
test_data = cc_data[-split_sample,]
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100, distance =1, scale = TRUE)
summary(Loocv_model)
#run the accuracy check for train_data.
predicted_train <- rep(0,(nrow(train_data))) # predictions: start with a vector of all zeros
train_accuracy<- 0  #initialize variable
for (i in 1:nrow(train_data)){
model=kknn(V11~.,train_data[-i,],train_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_train[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
train_accuracy<- sum(predicted_train == train_data[,11]) / nrow(train_data)
#the result of train data with best k=32.
train_accuracy
predicted_test <- rep(0,(nrow(test_data))) # predictions: start with a vector of all zeros
test_accuracy<- 0 #initialize variable
for (i in 1:nrow(test_data)){
model=kknn(V11~.,test_data[-i,],test_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_test[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
test_accuracy<- sum(predicted_test == test_data[,11]) / nrow(test_data)
#the result of test data with best k=8.
test_accuracy
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
splitsample <- createDataPartition(cc_data$V1, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
trainData = cc_data[splitsample,]
#Assign the remaining data set for 20% of the original data
remainData = cc_data[-splitsample,]
#Generate a half of random sample of the remaining rows in remaining Data
splitsample2 <-sample(1:nrow(remainData),as.integer(0.5*nrow(remainData)))
#Assign the validation data set for 10% of the remaining data
valiData <- remainData[splitsample2,]
#Assgin the test data set for 10% of the remaining data
testData <- remainData[-splitsample2,]
#get the best k value for accuracy training.
loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100,
distance = 2, scale = TRUE)
summary(loocv_model)
#run knn with validation data set
kmax<-100
accuracy <- rep(0,kmax)
for (i in 1:kmax) {
# fit k-nearest-neighbor model using training set, validate on validation set
knn_model <- kknn(V11~.,trainData,valiData ,k=i,kernel = 'optimal',scale=TRUE)
#  compare models using validation set
pred <- as.integer(fitted(knn_model)+0.5) # round off to 0 or 1
accuracy[i] = sum(pred == valiData[,11]) / nrow(valiData)
}
#find the hightest accuracy
kknn_result <- data.frame(accuracy)
View(kknn_result)
#find the best k value
best_kvalue <- which.max(accuracy)
View(best_kvalue)
print(paste0("The highest kknn accuracy rate is ", max(accuracy),
" and the best k value is ", best_kvalue))
#run knn with test data set using the best k=7 and optimal kernel we had from the validation set.
knnmodel <- kknn(V11~.,trainData,testData,k=7,kernel = 'optimal',scale=TRUE)
pred <- as.integer(fitted(knnmodel)+0.5) # round off to 0 or 1
#get the accuracy for the test data by using the best k value from validation data set
test_accuracy = sum(pred == testData[,11]) / nrow(testData)
test_accuracy
print(paste0("We found the model accuracy rate for test dataset is ", test_accuracy))
#load the data into a table
iris_data <- read.table("iris.txt", stringsAsFactors = FALSE, header = TRUE)
tail(iris_data)
#Removed species to create cluster.
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = scaled_data) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = scaled_data) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = scaled_data) + ggtitle("k = 5")
knitr::opts_chunk$set(echo = TRUE)
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
library(gridExtra)
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
set.seed(10)
#Removed species to create cluster.
scaled_data <- scale(iris_data[1:4] )
#Plot Elbow
fviz_nbclust(scaled_data, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 6)
set.seed(10)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
library(gridExtra)
#Plotting clusters
cluster_plot <- grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
cluster_plot
knitr::opts_chunk$set(echo = FALSE)
library(gridExtra)
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
library(gridExtra)
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
set.seed(10)
#Removed species to create cluster.
scaled_data <- scale(iris_data[1:4] )
#Plot Elbow
fviz_nbclust(scaled_data, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 6)
