for (i in 1:nrow(train_data)){
model=kknn(V11~.,train_data[-i,],train_data[i,],k=30,kernel="optimal", scale = TRUE) # use scaled data
predicted_train[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
train_accuracy<- sum(predicted_train == train_data[,11]) / nrow(train_data)
#the result of train data with best k=8.
train_accuracy
predicted_test <- rep(0,(nrow(test_data))) # predictions: start with a vector of all zeros
test_accuracy<- 0 #initialize variable
for (i in 1:nrow(test_data)){
model=kknn(V11~.,test_data[-i,],test_data[i,],k=30,kernel="optimal", scale = TRUE) # use scaled data
predicted_test[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
test_accuracy<- sum(predicted_test == test_data[,11]) / nrow(test_data)
#the result of test data with best k=8.
test_accuracy
?as.factor
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
library(scales)
library(kableExtra)
library(knitr)
#library(quantreg)
library(corrplot)
library(gridExtra)
library(NbClust)
library(ISLR)
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
#library(scales)
#library(kableExtra)
library(knitr)
#library(quantreg)
library(corrplot)
#library(gridExtra)
library(NbClust)
library(ISLR)
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
setwd("~/Desktop/hw2-FA23/data 3.1")
#import library
library(kknn)
library(tidyr)
library(dplyr)
library(lattice)
library(caret)
library(readr)
library(ggplot2)
library(tidyverse)
library(kernlab)
#library(scales)
#library(kableExtra)
library(knitr)
#library(quantreg)
library(corrplot)
#library(gridExtra)
library(NbClust)
library(ISLR)
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
split_sample <- createDataPartition(cc_data$V11, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
train_data = cc_data[split_sample,]
#Assign the testData set to the remaining 20% of the original data
test_data = cc_data[-split_sample,]
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn(formula = V11~., data = train_data, kmax = 30, distance =1, scale = TRUE)
summary(Loocv_model)
#run the accuracy check for train_data.
predicted_train <- rep(0,(nrow(train_data))) # predictions: start with a vector of all zeros
train_accuracy<- 0  #initialize variable
for (i in 1:nrow(train_data)){
model=kknn(V11~.,train_data[-i,],train_data[i,],k=30,kernel="optimal", scale = TRUE) # use scaled data
predicted_train[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
train_accuracy<- sum(predicted_train == train_data[,11]) / nrow(train_data)
#the result of train data with best k=8.
train_accuracy
#---predict the test data accuracy using the same formula---#
predicted_test <- rep(0,(nrow(test_data))) # predictions: start with a vector of all zeros
test_accuracy<- 0 #initialize variable
for (i in 1:nrow(test_data)){
model=kknn(V11~.,test_data[-i,],test_data[i,],k=30,kernel="optimal", scale = TRUE) # use scaled data
predicted_test[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
test_accuracy<- sum(predicted_test == test_data[,11]) / nrow(test_data)
#the result of test data with best k=8.
test_accuracy
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 30, distance =1, scale = TRUE)
summary(Loocv_model)
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100, distance =1, scale = TRUE)
summary(Loocv_model)
#run the accuracy check for train_data.
predicted_train <- rep(0,(nrow(train_data))) # predictions: start with a vector of all zeros
train_accuracy<- 0  #initialize variable
for (i in 1:nrow(train_data)){
model=kknn(V11~.,train_data[-i,],train_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_train[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
train_accuracy<- sum(predicted_train == train_data[,11]) / nrow(train_data)
#the result of train data with best k=8.
train_accuracy
#---predict the test data accuracy using the same formula---#
predicted_test <- rep(0,(nrow(test_data))) # predictions: start with a vector of all zeros
test_accuracy<- 0 #initialize variable
for (i in 1:nrow(test_data)){
model=kknn(V11~.,test_data[-i,],test_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_test[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
test_accuracy<- sum(predicted_test == test_data[,11]) / nrow(test_data)
#the result of test data with best k=8.
test_accuracy
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
splitsample <- createDataPartition(cc_data$V1, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
trainData = cc_data[splitsample,]
#Assign the remaining data set for 20% of the original data
remainData = cc_data[-splitsample,]
#Generate a half of random sample of the remaining rows in remaining Data
splitsample2 <-sample(1:nrow(remainData),as.integer(0.5*nrow(remainData)))
#Assign the validation data set for 10% of the remaining data
valiData <- remainData[splitsample2,]
#Assgin the test data set for 10% of the remaining data
testData <- remainData[-splitsample2,]
#get the best k value for accuracy training.
loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100,
distance = 2, scale = TRUE)
summary(loocv_model)
#run knn with validation data set
kmax<-100
accuracy <- rep(0,kmax)
for (i in 1:kmax) {
# fit k-nearest-neighbor model using training set, validate on validation set
knn_model <- kknn(V11~.,trainData,valiData ,k=i,kernel = 'optimal',scale=TRUE)
#  compare models using validation set
pred <- as.integer(fitted(knn_model)+0.5) # round off to 0 or 1
accuracy[i] = sum(pred == valiData[,11]) / nrow(valiData)
}
#find the hightest accuracy
kknn_result <- data.frame(accuracy)
View(kknn_result)
#find the best k value
best_kvalue <- which.max(accuracy)
View(best_kvalue)
print(paste0("The highest kknn accuracy rate is ", max(accuracy),
" and the best k value is ", best_kvalue))
#Plot the graph for accuracy vs. K-values
par(bg="white")
plot(accuracy,ylab="accuracy level",xlab="K values",type='b',col='blue', main='KKN accuracy')
#run knn with test data set using the best k=7 from the validation set.
knnmodel <- kknn(V11~.,trainData,testData,k=7,kernel = 'optimal',scale=TRUE)
pred <- as.integer(fitted(knnmodel)+0.5) # round off to 0 or 1
#get the accuracy for the test data by using the best k value from validation data set
test_accuracy = sum(pred == testData[,11]) / nrow(testData)
test_accuracy
print(paste0("We found the model accuracy rate for test dataset is ", test_accuracy))
#library(cluster)    # clustering algorithms
#library(factoextra) # clustering algorithms & visualization
iris_data <- read.table("iris.txt", stringsAsFactors = FALSE, header = TRUE)
tail(iris_data)
#Scaling the predictors
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
#Question 3.1 B
#create a random sample and reproducible.
set.seed(111)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
splitsample <- createDataPartition(cc_data$V1, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
trainData = cc_data[splitsample,]
#Assign the remaining data set for 20% of the original data
remainData = cc_data[-splitsample,]
#Generate a half of random sample of the remaining rows in remaining Data
splitsample2 <-sample(1:nrow(remainData),as.integer(0.5*nrow(remainData)))
#Assign the validation data set for 10% of the remaining data
valiData <- remainData[splitsample2,]
#Assgin the test data set for 10% of the remaining data
testData <- remainData[-splitsample2,]
#get the best k value for accuracy training.
loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100,
distance = 2, scale = TRUE)
summary(loocv_model)
#run knn with validation data set
kmax<-100
accuracy <- rep(0,kmax)
for (i in 1:kmax) {
# fit k-nearest-neighbor model using training set, validate on validation set
knn_model <- kknn(V11~.,trainData,valiData ,k=i,kernel = 'optimal',scale=TRUE)
#  compare models using validation set
pred <- as.integer(fitted(knn_model)+0.5) # round off to 0 or 1
accuracy[i] = sum(pred == valiData[,11]) / nrow(valiData)
}
#find the hightest accuracy
kknn_result <- data.frame(accuracy)
View(kknn_result)
#find the best k value
best_kvalue <- which.max(accuracy)
View(best_kvalue)
print(paste0("The highest kknn accuracy rate is ", max(accuracy),
" and the best k value is ", best_kvalue))
#Plot the graph for accuracy vs. K-values
par(bg="white")
plot(accuracy,ylab="accuracy level",xlab="K values",type='b',col='blue', main='KKN accuracy')
#run knn with test data set using the best k=7 from the validation set.
knnmodel <- kknn(V11~.,trainData,testData,k=7,kernel = 'optimal',scale=TRUE)
pred <- as.integer(fitted(knnmodel)+0.5) # round off to 0 or 1
#get the accuracy for the test data by using the best k value from validation data set
test_accuracy = sum(pred == testData[,11]) / nrow(testData)
test_accuracy
print(paste0("We found the model accuracy rate for test dataset is ", test_accuracy))
#Scaling the predictors
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
install.packages("factoextra")
library(factoextra)
#Scaling the predictors
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = scaled_data) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = scaled_data) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = scaled_data) + ggtitle("k = 5")
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Fig1: Cluster Plots")
#Scaling the predictors
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = scaled_data) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = scaled_data) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = scaled_data) + ggtitle("k = 5")
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
table(k2$cluster, iris_data$Species)
table(k3$cluster, iris_data$Species)
table(k4$cluster, iris_data$Species)
table(k5$cluster, iris_data$Species)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled.data )
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='upper',bg="lightblue")
mtext("Fig3: Correlation Matrix", at=-.3, line=-14, cex=1.)
iris_data.cor = cor(iris_data)
library(corrplot)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='upper',bg="lightblue")
mtext("Correlation Matrix", at=-.3, line=-14, cex=1.)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="lightblue")
mtext("Correlation Matrix", at=-.3, line=-14, cex=1.)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='full',bg="lightblue")
mtext("Correlation Matrix", at=-.3, line=-14, cex=1.)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="lightblue")
mtext("Correlation Matrix", at=-.3, line=-14, cex=1.)
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="lightblue")
set.seed(2)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="ACFFFE")
set.seed(1234)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="lightgreen")
set.seed(1234)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="peachpuff")
set.seed(1234)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="lemonchiffon")
set.seed(1234)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
fviz_nbclust(iris_data, kmeans, method ="wss")
#Convert species names to numbers
mapping<- c("setosa"=1,"versicolor"=2,"virginica"=3)
iris_data$Species<- mapping[iris_data$Species]
#remove species column for clustering
idata<-iris_data[,1:4]
fviz_nbclust(idata, kmeans, method ="wss")
View(iris_data)
#remove species column for clustering
idata<-iris_data[,1:4]
#fviz_nbclust(idata, kmeans, method ="wss")
fviz_nbclust(idata, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 2)
set.seed(10)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
set.seed(10)
#remove species column for clustering
idata<-iris_data[,1:4]
#fviz_nbclust(idata, kmeans, method ="wss")
fviz_nbclust(idata, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 2)
#fviz_nbclust(idata, kmeans, method ="wss")
fviz_nbclust(idata, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 6)
#fviz_nbclust(idata, kmeans, method ="wss")
fviz_nbclust(scaled_data, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 6)
set.seed(10)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100, distance =2, scale = TRUE)
summary(Loocv_model)
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100, distance =1, scale = TRUE)
summary(Loocv_model)
#import library
library(kknn)
library(ggplot2)
library(lattice)
library(caret)
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
split_sample <- createDataPartition(cc_data$V11, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
train_data = cc_data[split_sample,]
#Assign the testData set to the remaining 20% of the original data
test_data = cc_data[-split_sample,]
#Use LOOCV to determind best K- value
Loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100, distance =1, scale = TRUE)
summary(Loocv_model)
#run the accuracy check for train_data.
predicted_train <- rep(0,(nrow(train_data))) # predictions: start with a vector of all zeros
train_accuracy<- 0  #initialize variable
for (i in 1:nrow(train_data)){
model=kknn(V11~.,train_data[-i,],train_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_train[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
train_accuracy<- sum(predicted_train == train_data[,11]) / nrow(train_data)
#the result of train data with best k=8.
train_accuracy
#---predict the test data accuracy using the same formula---#
#Run optimal kernel and best k=32 into test dataset.
predicted_test <- rep(0,(nrow(test_data))) # predictions: start with a vector of all zeros
test_accuracy<- 0 #initialize variable
for (i in 1:nrow(test_data)){
model=kknn(V11~.,test_data[-i,],test_data[i,],k=32,kernel="optimal", scale = TRUE) # use scaled data
predicted_test[i]<- as.integer(fitted(model)+0.5) # round off to 0 or 1 and store predicted values in vector
}
# calculate fraction of correct predictions
test_accuracy<- sum(predicted_test == test_data[,11]) / nrow(test_data)
#the result of test data with best k=32.
test_accuracy
#create a random sample and reproducible.
set.seed(1234)
#load data from txt doc.
cc_data <- read.table("credit_card_data.txt", stringsAsFactors = FALSE, header = FALSE)
head(cc_data)
#Generate a random sample of 80% of the rows
splitsample <- createDataPartition(cc_data$V1, p = 0.8, list = FALSE)
#Assign the train data set to 80% of the orginal data
trainData = cc_data[splitsample,]
#Assign the remaining data set for 20% of the original data
remainData = cc_data[-splitsample,]
#Generate a half of random sample of the remaining rows in remaining Data
splitsample2 <-sample(1:nrow(remainData),as.integer(0.5*nrow(remainData)))
#Assign the validation data set for 10% of the remaining data
valiData <- remainData[splitsample2,]
#Assgin the test data set for 10% of the remaining data
testData <- remainData[-splitsample2,]
#get the best k value for accuracy training.
loocv_model <- train.kknn((V11)~., data = train_data, kmax = 100,
distance = 2, scale = TRUE)
summary(loocv_model)
#run knn with validation data set
kmax<-100
accuracy <- rep(0,kmax)
for (i in 1:kmax) {
# fit k-nearest-neighbor model using training set, validate on validation set
knn_model <- kknn(V11~.,trainData,valiData ,k=i,kernel = 'optimal',scale=TRUE)
#  compare models using validation set
pred <- as.integer(fitted(knn_model)+0.5) # round off to 0 or 1
accuracy[i] = sum(pred == valiData[,11]) / nrow(valiData)
}
#find the hightest accuracy
kknn_result <- data.frame(accuracy)
View(kknn_result)
#find the best k value
best_kvalue <- which.max(accuracy)
View(best_kvalue)
print(paste0("The highest kknn accuracy rate is ", max(accuracy),
" and the best k value is ", best_kvalue))
#Plot the Elbow graph for accuracy vs. K-values
par(bg="white")
plot(accuracy,ylab="accuracy level",xlab="K values",type='b',col='blue', main='KKN accuracy')
#run knn with test data set using the best k=7 and optimal kernel we had from the validation set.
knnmodel <- kknn(V11~.,trainData,testData,k=7,kernel = 'optimal',scale=TRUE)
pred <- as.integer(fitted(knnmodel)+0.5) # round off to 0 or 1
#get the accuracy for the test data by using the best k value from validation data set
test_accuracy = sum(pred == testData[,11]) / nrow(testData)
test_accuracy
print(paste0("We found the model accuracy rate for test dataset is ", test_accuracy))
#import library
library(ggplot2)
library(tidyverse)
library(kernlab)
library(knitr)
library(corrplot)
library(NbClust)
library(ISLR)
library(factoextra)
#import data into table
iris_data <- read.table("iris.txt", stringsAsFactors = FALSE, header = TRUE)
tail(iris_data)
#Removed species to create cluster.
scaled_data <- scale(iris_data[1:4] )
k2 <- kmeans(scaled_data, centers = 2, nstart = 25)
k3 <- kmeans(scaled_data, centers = 3, nstart = 25)
k4 <- kmeans(scaled_data, centers = 4, nstart = 25)
k5 <- kmeans(scaled_data, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = scaled_data) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = scaled_data) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = scaled_data) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = scaled_data) + ggtitle("k = 5")
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
#Plot Elbow graph to find the best k value
fviz_nbclust(scaled_data, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 6)
#correlation between independent variables
corrmatrix <- cor(scaled_data )
corrplot(corrmatrix, method = 'number',type='lower',bg="slategray1")
#Plotting clusters
grid.arrange(p1, p2, p3, p4, nrow = 2, top="Cluster Plots")
rmarkdown::render
setwd("~/Desktop/hw2-FA23/data 3.1")
